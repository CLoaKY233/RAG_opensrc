# ğŸ¤– RAG with Mistral - Local LLM Question Answering System

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B.svg)](https://streamlit.io/)

A powerful Retrieval-Augmented Generation (RAG) system built with Mistral LLM, offering local document processing and intelligent question answering capabilities.

## ğŸ“‘ Table of Contents

- [Features](#-features)
- [Prerequisites](#-prerequisites)
- [Installation](#-installation)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Documentation](#-documentation)
- [Learning Resources](#-learning-resources)
- [Contributing](#-contributing)
- [Troubleshooting](#-troubleshooting)
- [Contact](#-contact)
- [License](#-license)

## âœ¨ Features

- ğŸ“š Local document processing (PDF, TXT, DOCX)
- ğŸ” Intelligent text chunking and embedding
- ğŸ’¡ Context-aware question answering
- ğŸ¯ Customizable response parameters
- ğŸ–¥ï¸ User-friendly Streamlit interface

## ğŸ”§ Prerequisites

- Python 3.8 or higher
- Git
- [Ollama](https://ollama.ai/) with Mistral model installed

## ğŸ“¥ Installation

1. Clone the repository:
```bash
git clone https://github.com/cloaky233/RAG_Opensrc.git
cd RAG_Opensrc
```

2. Create and activate a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Install Mistral model in Ollama:
```bash
ollama pull mistral
```

## ğŸš€ Usage

1. Start the Streamlit application:
```bash
streamlit run stmain.py
```

2. Access the web interface at `http://localhost:8501`

3. Upload documents and start asking questions!

### Basic Workflow:
1. Navigate to the "Documents" tab
2. Upload your documents
3. Process the documents
4. Switch to the "Chat" tab
5. Start asking questions about your documents

## ğŸ“ Project Structure

```
RAG_Opensrc/
    â”œâ”€â”€ pysrc/
    â”‚   â”œâ”€â”€ document_processor.py
    â”‚   â”œâ”€â”€ embeddings_manager.py
    â”‚   â””â”€â”€ rag_engine.py
    â”œâ”€â”€ stmain.py
    â”œâ”€â”€ Info.md
    â”œâ”€â”€ LearnWithPrompts.md
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ README.md
```

## ğŸ“š Documentation

For detailed information about the project architecture and implementation details, please refer to:
- [Info.md](Info.md) - Technical documentation and system architecture
- [LearnWithPrompts.md](LearnWithPrompts.md) - Interactive learning guide with examples

## ğŸ“– Learning Resources

New to RAG systems? Check out our learning resources:
1. Review [LearnWithPrompts.md](LearnWithPrompts.md) for step-by-step tutorials
2. Explore the [Info.md](Info.md) for technical insights
3. Try the example prompts provided in the documentation

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request


## â— Troubleshooting

Common issues and solutions:

1. **Ollama Connection Error**
   - Ensure Ollama is running locally
   - Verify Mistral model is installed

2. **Document Processing Issues**
   - Check file formats (PDF, TXT, DOCX only)
   - Ensure files are not corrupted

For more issues, please check our [Issues](https://github.com/cloaky233/RAG_Opensrc/issues) page.

## ğŸ“« Contact



Project Link: [https://github.com/cloaky233/RAG_Opensrc](https://github.com/cloaky233/RAG_Opensrc)

## ğŸ“„ License

Distributed under the MIT License. See `LICENSE` for more information.

---

â­ Star this repo if you find it helpful!
